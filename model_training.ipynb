{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attendance System – Model Training\n",
    "\n",
    "This notebook trains an emotion-detection CNN (FER2013) and builds a face-recognition embedding database for the automated attendance project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pawar\\attendance_env\\lib\\site-packages\\face_recognition_models\\__init__.py:7: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.0\n",
      "GPU Available: []\n"
     ]
    }
   ],
   "source": [
    "# Section 1 – Imports & Environment Check\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mtcnn import MTCNN\n",
    "import face_recognition\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "print('GPU Available:', tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset not found at fer2013_images\n",
      "Please download FER2013 dataset from Kaggle\n"
     ]
    }
   ],
   "source": [
    "# Section 2 – Load & Pre-process FER2013 Dataset\n",
    "def load_fer2013_data(data_path='fer2013_images'):\n",
    "    \"\"\"Load and preprocess FER2013 dataset\"\"\"\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Dataset not found at {data_path}\")\n",
    "        print(\"Please download FER2013 dataset from Kaggle\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Read the dataset\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Emotion distribution:\\n{df['emotion'].value_counts()}\")\n",
    "\n",
    "    # Emotion labels\n",
    "    emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "\n",
    "    # Convert pixel strings to numpy arrays\n",
    "    def pixels_to_array(pixel_string):\n",
    "        return np.array(pixel_string.split(), dtype=np.float32).reshape(48, 48, 1)\n",
    "\n",
    "    # Process images\n",
    "    X = np.array([pixels_to_array(pixels) for pixels in tqdm(df['pixels'], desc='Processing images')])\n",
    "    y = df['emotion'].values\n",
    "\n",
    "    # Normalize pixel values\n",
    "    X = X / 255.0\n",
    "\n",
    "    # Convert labels to categorical\n",
    "    y = keras.utils.to_categorical(y, 7)\n",
    "\n",
    "    # Split data (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Load the FER2013 data\n",
    "X_train, X_test, y_train, y_test = load_fer2013_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\pawar\\attendance_env\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\pawar\\attendance_env\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\pawar\\attendance_env\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 46, 46, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 46, 46, 32)        128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 44, 44, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 22, 22, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 22, 22, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 20, 20, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 20, 20, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 18, 18, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 9, 9, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 9, 9, 64)          0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 7, 7, 128)         512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 5, 5, 128)         147584    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 2, 2, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               262656    \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 685159 (2.61 MB)\n",
      "Trainable params: 683687 (2.61 MB)\n",
      "Non-trainable params: 1472 (5.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Section 3 – Emotion Detection Model Architecture\n",
    "def create_emotion_model():\n",
    "    \"\"\"Create CNN model for emotion detection\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # First Convolutional Block\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        # Second Convolutional Block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        # Third Convolutional Block\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(7, activation='softmax')  # 7 emotions\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the emotion detection model\n",
    "emotion_model = create_emotion_model()\n",
    "emotion_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Skipping training - dataset not available\n"
     ]
    }
   ],
   "source": [
    "# Section 4 – Train Emotion Detection Model\n",
    "if X_train is not None:\n",
    "    # Training callbacks\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5),\n",
    "        keras.callbacks.ModelCheckpoint('models/emotion_model_best.h5', save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = emotion_model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=50,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Save the final model\n",
    "    emotion_model.save('models/emotion_model.h5')\n",
    "    print(\" Emotion model saved successfully\")\n",
    "else:\n",
    "    print(\" Skipping training - dataset not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5 – Model Evaluation\n",
    "if X_train is not None:\n",
    "    # Evaluate the model\n",
    "    test_loss, test_accuracy = emotion_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f'Test accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('models/training_history.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student images directory not found: data/students\n",
      "Please create the directory and add student photos\n"
     ]
    }
   ],
   "source": [
    "# Section 6 – Face Recognition Setup\n",
    "def create_face_embeddings_database(student_images_path='data/students'):\n",
    "    \"\"\"Create face embeddings database for student recognition\"\"\"\n",
    "    face_embeddings = {}\n",
    "    student_info = {}\n",
    "\n",
    "    if not os.path.exists(student_images_path):\n",
    "        print(f\"Student images directory not found: {student_images_path}\")\n",
    "        print(\"Please create the directory and add student photos\")\n",
    "        return face_embeddings, student_info\n",
    "\n",
    "    # Process each student directory\n",
    "    for student_dir in os.listdir(student_images_path):\n",
    "        student_path = os.path.join(student_images_path, student_dir)\n",
    "        if not os.path.isdir(student_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing student: {student_dir}\")\n",
    "        student_embeddings = []\n",
    "\n",
    "        # Process each image in student directory\n",
    "        for img_file in os.listdir(student_path):\n",
    "            if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                img_path = os.path.join(student_path, img_file)\n",
    "\n",
    "                # Load and process image\n",
    "                image = face_recognition.load_image_file(img_path)\n",
    "                face_encodings = face_recognition.face_encodings(image)\n",
    "\n",
    "                if face_encodings:\n",
    "                    student_embeddings.append(face_encodings[0])\n",
    "                    print(f\" Processed: {img_file}\")\n",
    "                else:\n",
    "                    print(f\"  No face found in: {img_file}\")\n",
    "\n",
    "        if student_embeddings:\n",
    "            # Store average embedding for the student\n",
    "            avg_embedding = np.mean(student_embeddings, axis=0)\n",
    "            face_embeddings[student_dir] = avg_embedding\n",
    "\n",
    "            # Extract student info from directory name (format: ID_Name)\n",
    "            if '_' in student_dir:\n",
    "                student_id, student_name = student_dir.split('_', 1)\n",
    "                student_info[student_dir] = {\n",
    "                    'id': student_id,\n",
    "                    'name': student_name.replace('_', ' ')\n",
    "                }\n",
    "            else:\n",
    "                student_info[student_dir] = {\n",
    "                    'id': student_dir,\n",
    "                    'name': student_dir\n",
    "                }\n",
    "\n",
    "    # Save embeddings\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    with open('models/face_embeddings.pkl', 'wb') as f:\n",
    "        pickle.dump(face_embeddings, f)\n",
    "\n",
    "    with open('models/student_info.pkl', 'wb') as f:\n",
    "        pickle.dump(student_info, f)\n",
    "\n",
    "    print(f\" Face embeddings created for {len(face_embeddings)} students\")\n",
    "    return face_embeddings, student_info\n",
    "\n",
    "# Create the face embeddings database\n",
    "face_embeddings, student_info = create_face_embeddings_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\pawar\\attendance_env\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:189: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Section 7 – Test Face Detection Pipeline\n",
    "detector = MTCNN()\n",
    "\n",
    "def test_face_detection(image_path):\n",
    "    \"\"\"Test face detection on a sample image\"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Test image not found: {image_path}\")\n",
    "        return\n",
    "\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = detector.detect_faces(rgb_image)\n",
    "\n",
    "    print(f\"Detected {len(faces)} face(s)\")\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    for face in faces:\n",
    "        x, y, width, height = face['box']\n",
    "        cv2.rectangle(image, (x, y), (x + width, y + height), (255, 0, 0), 2)\n",
    "\n",
    "        # Add confidence text\n",
    "        confidence = face['confidence']\n",
    "        cv2.putText(image, f'{confidence:.2f}', (x, y-10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "    # Display result\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Face Detection Test')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Uncomment to test with a sample image:\n",
    "# test_face_detection('data/test_image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MODEL TRAINING SUMMARY ===\n",
      " Files created:\n",
      " - models/emotion_model.h5 (Emotion detection model)\n",
      " - models/emotion_model_best.h5 (Best checkpoint)\n",
      " - models/face_embeddings.pkl (Face recognition database)\n",
      " - models/student_info.pkl (Student information)\n",
      " - models/training_history.png (Training plots)\n",
      "\n",
      " Next steps:\n",
      " 1. Add student photos to data/students/ directory\n",
      " 2. Run the main attendance system\n",
      " 3. Configure time window (9:30 AM - 10:00 AM) in config.py\n",
      "\n",
      " Note: If model files are large (>100MB), upload to Google Drive\n"
     ]
    }
   ],
   "source": [
    "# Section 8 – Model Export Summary\n",
    "print(\"\\n=== MODEL TRAINING SUMMARY ===\")\n",
    "print(\" Files created:\")\n",
    "print(\" - models/emotion_model.h5 (Emotion detection model)\")\n",
    "print(\" - models/emotion_model_best.h5 (Best checkpoint)\")\n",
    "print(\" - models/face_embeddings.pkl (Face recognition database)\")\n",
    "print(\" - models/student_info.pkl (Student information)\")\n",
    "print(\" - models/training_history.png (Training plots)\\n\")\n",
    "print(\" Next steps:\")\n",
    "print(\" 1. Add student photos to data/students/ directory\")\n",
    "print(\" 2. Run the main attendance system\")\n",
    "print(\" 3. Configure time window (9:30 AM - 10:00 AM) in config.py\\n\")\n",
    "print(\" Note: If model files are large (>100MB), upload to Google Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attendance_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
